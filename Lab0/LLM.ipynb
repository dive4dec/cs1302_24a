{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65721807",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "title: LLM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13a4f0",
   "metadata": {},
   "source": [
    "::::{attention}\n",
    "\n",
    "This notebook is optional and NOT required for any course assessment activities. Lab tutor may go through them if time is available.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8cd87",
   "metadata": {},
   "source": [
    "## Integration with Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45428e",
   "metadata": {},
   "source": [
    "You can play with Large language models (LLMs) in various ways. LLMs have been integrated into the Jupyter server using [`jupyter-ai`](https://github.com/jupyterlab/jupyter-ai/blob/main/README.md). To be able to chat with an LLM inside a notebook, you need to run the {term}`line magic` `%load_ext jupyter_ai`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa89f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "%load_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535db2f1",
   "metadata": {},
   "source": [
    "As an example, run the following {term}`cell magic` to ask an LLM what an LLM is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd36677",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive:chat\n",
    "Explain what is LLM in one line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92507d19",
   "metadata": {},
   "source": [
    "Let's try different output formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive:chat -f math\n",
    "What is the Pythagoras theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140c3a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "%%ai dive:chat -f html\n",
    "What is the Pythagoras theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d1e0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "%%ai dive:chat -f html\n",
    "Illustrate the Pythagoras theorem using a diagram in SVG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive:chat -f text\n",
    "Can I trust you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac3b54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{caution}\n",
    "\n",
    "As LLM can hallucinates and produce wrong answers, do NOT copy from it directly. You should verify the correctness in different ways.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74571ef",
   "metadata": {},
   "source": [
    "::::{seealso} Are there different LLMs?\n",
    ":class: dropdown\n",
    "\n",
    "`dive:chat` is just one of many large language models you may choose. To list all supported chat models, you can run\n",
    "\n",
    "```\n",
    "%ai list\n",
    "```\n",
    "\n",
    "For more information on selecting and customizing a model, run \n",
    "\n",
    "```\n",
    "%ai --help\n",
    "```\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115318d",
   "metadata": {},
   "source": [
    "There is also a [Jupyternaut](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#the-chat-interface) interface to chat with an LLM separately from the notebook.\n",
    "\n",
    "1. Click the chat icon <i class=\"fa-duotone fa-solid fa-messages\"></i> <i class=\"fas fa-comments\"></i> on the left menu bar. A chat panel will open.\n",
    "2. Enter some messages to see a response from the Azure OpenAI chat model.\n",
    "\n",
    "::::{tip} Trouble-shooting\n",
    ":class: dropdown\n",
    "\n",
    "If the above fail, reconfigure the model as follows:\n",
    "1. Click the gear icon <i class=\"fas fa-cog\"></i> at the top right-hand corner of the chat panel.\n",
    "3. Select the Completion model as `DIVE :: chat`.\n",
    "5. Click <kbd>Save Changes</kbd> button.\n",
    "6. Click the back arrow at the top to go back to the chat panel.\n",
    "7. Enter some messages to see a response from the chat model.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9713fd8",
   "metadata": {},
   "source": [
    "### Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b74b3",
   "metadata": {},
   "source": [
    "To play with a bigger LLM model such as <wiki:GPT-4o>, registered students can subscribe to [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service) as follows:\n",
    "\n",
    "1. Access the [APIM portal](https://apim-aoai-eas-dev02.developer.azure-api.net) and click `Active Directory - Login` to login with your CityU active directory credentials.\n",
    "2. From the top menu, navigate to the `Products` page and click the link to `cs-dive`\n",
    "3. Enter any name such as `aitutor` in the textbox and click <kbd>subscribe</kbd>. You will be brought to your profile page where you can show/regenerate your primary/secondary API keys.\n",
    "4. Copy one of the API keys such as the `Primary key` for the subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fdea3",
   "metadata": {},
   "source": [
    "::::{note} Why API keys?\n",
    ":class: dropdown\n",
    "\n",
    "API keys are used to restrict access to OpenAI large language models (LLMs) to ensure that only authorized users can utilize the service and its resources. By issuing unique API keys, the service providers can track and control usage, enforce rate limits, and prevent unauthorized access, which is crucial for maintaining security and managing resource allocation. Over time, an API key can become compromised or leaked, potentially leading to misuse, such as exceeding the allocated quota limit or incurring unexpected costs. Regularly regenerating the API key helps mitigate these risks by invalidating the old key and issuing a new one, thus effectively cutting off any unauthorized access that might have occurred due to the key's exposure. This practice ensures continued security and optimal usage of the LLM service by authorized users. To regenerate your keys:\n",
    "\n",
    "1. Access the [Profile](https://apim-aoai-eas-dev02.developer.azure-api.net/profile) page on the APIM portal.\n",
    "2. Click the `Regenerate` link next to the respective key to regenerates the key.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f26c3d",
   "metadata": {},
   "source": [
    "Next, select and configure the Azure OpenAI model as follows:\n",
    "\n",
    "1. Click the chat icon <i class=\"fas fa-comments\"></i> on the left menu bar. A chat panel will open.\n",
    "2. Click the gear icon <i class=\"fas fa-cog\"></i> on the chat panel to set up the provider.\n",
    "3. Select the Completion model as `DIVE Azure :: gpt4o`.\n",
    "4. Enter an API key to the `AZURE_OPEN_API_KEY` field. Other fields can be left empty.\n",
    "5. Save the API Key and click the `Save Changes` button.\n",
    "6. Click the back arrow at the top to go back to the chat window.\n",
    "7. Enter some messages to see a response from the Azure OpenAI chat model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b059591",
   "metadata": {},
   "source": [
    "If you want to reset the configuration in case of error, remove the configuration folder using the {term}`shell capture` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a8ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input('Reset Jupyter AI configuration? [y/N]').lower() == 'y':\n",
    "    !rm -rf ~/.local/share/jupyter/jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f94b4",
   "metadata": {},
   "source": [
    "After deleting the folder, simply restart the Jupyter server to recreate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824917a",
   "metadata": {},
   "source": [
    "To use the model in a cell magic, run the following to record the API as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dive_azure_openai import set_api_key\n",
    "\n",
    "set_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e867abd",
   "metadata": {},
   "source": [
    "::::{seealso} LinkedIn Learning\n",
    "\n",
    "- [Exploring environment variables and PATH][1]\n",
    "\n",
    "::::\n",
    "\n",
    "[1]: https://www.linkedin.com/learning-login/share?account=76816450&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Flearning-linux-command-line-14447912%2Fexploring-environment-variables-and-path%3Ftrk%3Dshare_video_url%26shareId%3Dv5xEB5VMRdevpyIFeyrDkQ%253D%253D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive-azure:gpt4o -f math\n",
    "Give an elegant proof of the Pythagoras theorem in LaTeX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92449486",
   "metadata": {},
   "source": [
    "To use the model in a python program, create an AzureOpenAI client, which will automatically takes the the subscription information from the environment variables:[^env]\n",
    "\n",
    "[^env]: In addition to `AZURE_OPENAI_API_KEY`, it also take the environment variables `AZURE_OPENAI_ENDPOINT` and `OPENAI_API_VERSION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "deployment_name = \"gpt4o\"\n",
    "client = AzureOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1bc1f",
   "metadata": {},
   "source": [
    "Create a chat completion to receive a reply to a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4920fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI tutor for introductory-level programming who give hints but not answers.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Human use it to talk to the computer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is REST API?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbac6db",
   "metadata": {},
   "source": [
    "::::{seealso} How does Jupyter access Azure OpenAI?\n",
    ":class: dropdown\n",
    "\n",
    "The communication is through {term}`REST API`. Accessing Azure OpenAI via REST API is like sending a letter (API request) to a highly knowledgeable friend (Azure OpenAI model) and getting a well-thought-out response back (API response) through the mail service (HTTP). You may play with the API on the [APIs page](https://apim-aoai-eas-dev02.developer.azure-api.net/apis) on APIM portal as follows:\n",
    "\n",
    "1. Navigate to the `APIs` page and click the link to `cs-eastus`.\n",
    "2. On the left panel, click `GPT-4o :: chat-completion-API` to show the corresponding [REST API endpoint](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions) for chat completion:\n",
    "   ```\n",
    "   https://apim-aoai-eas-dev02.azure-api.net/cs-eastus/openai/deployments/gpt4o/chat/completions\n",
    "   ```\n",
    "3. Click `Try it` to open the right panel.\n",
    "4. Click `+ Add parameter` and enter the [REST API version](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) with\n",
    "   - name:\n",
    "     ```\n",
    "     api-version\n",
    "     ```\n",
    "   - value:\n",
    "     ```\n",
    "     2024-02-01\n",
    "     ```\n",
    "5. In the textbox under `Body`, enter the message body in a format expected by the API such as:[^eg-restapi]\n",
    "   ```json\n",
    "   {\n",
    "       \"messages\": [\n",
    "           {\"role\": \"system\", \"content\": \"You are a helpful AI tutor for introductory-level programming who give hints but not answers.\"},\n",
    "           {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"Human use it to talk to the computer.\"},\n",
    "           {\"role\": \"user\", \"content\": \"What is REST API?\"}\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "::::\n",
    "\n",
    "[^eg-restapi]: See other examples [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#example-request-2) for chat completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d88722",
   "metadata": {},
   "source": [
    "::::{seealso} LinkedIn Learning\n",
    "\n",
    "- [Introduction to Conversational AI][1]\n",
    "\n",
    "::::\n",
    "\n",
    "[1]:https://www.linkedin.com/learning-login/share?account=76816450&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Fopenai-api-for-python-developers%2Fintroduction-to-conversational-ai%3Ftrk%3Dshare_video_url%26shareId%3DLsQaWke3T4uBAo4ecENWVw%253D%253D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff551b8",
   "metadata": {},
   "source": [
    "### local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a59836",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "We can run a generative model locally using [Ollama](https://ollama.com/). To do so, start the ollama service as follows.\n",
    "\n",
    "1. In JupyterLab, navigate to the `File` menu.\n",
    "1. Select `New` from the drop-down menu and choose `Terminal`.\n",
    "1. The terminal window will appear. You can use this terminal to run a shell command. Enter the following command into the terminal prompt and hit enter.\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "1. To terminate Ollama, simply type <kbd>Ctrl + C</kbd>, the same way you would terminate any shell command by Keyboard Interrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc397374",
   "metadata": {},
   "source": [
    "After Ollama is running, run the following cell to chat with a compact LLM model `phi3`.[^time-firstrun]\n",
    "\n",
    "[^time-firstrun]: The first run will take some time for loading the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('OLLAMA_MODELSS','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ffbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive-ollama:phi3 -f text\n",
    "When was your training cutoff date and how were you trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1344c",
   "metadata": {},
   "source": [
    "To use Ollama with Jupyternaut:\n",
    "\n",
    "1. Click the chat icon <i class=\"fas fa-comments\"></i> on the left menu bar. A chat panel will open.\n",
    "2. Click the gear icon <i class=\"fas fa-cog\"></i> on the chat panel to set up the provider.\n",
    "3. Select the Completion model as `DIVE Ollama :: ...` with `...` replaced by your desired model such as `phi3`.\n",
    "5. Click the `Save Changes` button.\n",
    "6. Click the back arrow at the top to go back to the chat panel.\n",
    "7. Enter some messages to see a response from the Azure OpenAI chat model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb444d",
   "metadata": {},
   "source": [
    "The following models would be too slow to run without GPU. If you are using the `Default` server option, consider restarting your server with a `GPU` server option.[^time-reload]\n",
    "\n",
    "[^time-reload]: Switching models require additional time to reload a new model into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c509c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive-ollama:codellama -f text\n",
    "When was your training cutoff date and how were you trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive-ollama:mistral -f text\n",
    "When was your training cutoff date and how were you trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai dive-ollama:dolphin-mistral -f text\n",
    "When was your training cutoff date and how were you trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8d8e3",
   "metadata": {},
   "source": [
    "Different models have different sizes and may be good at different things. The following executes the a shell command to list other models that can be served by ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6260af",
   "metadata": {},
   "source": [
    "The models reside in the directory specified by the environment variable `OLLAMA_MODELS`:[^custom]\n",
    "\n",
    "[^custom]: To download or run a new ollama model, you need to set the directory to `~/.ollama` or any directory you have write access to. To use the model in JupyterNaut, select the Completion model as `Ollama :: *` and specify the model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OLLAMA_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c5eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{seealso} LinkedIn Learning\n",
    "\n",
    "- [Installing the Llama 3 API with Ollama][1]\n",
    "\n",
    "::::\n",
    "\n",
    "[1]: https://www.linkedin.com/learning-login/share?account=76816450&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Fdeveloping-with-llama-3-meta-s-innovative-and-open-large-language-model%2Finstalling-the-llama-3-api-with-ollama%3Ftrk%3Dshare_video_url%26shareId%3D%252BqFRPLijRLaE%252FglfqzX5Sg%253D%253D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8fd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Integration with VSCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b3c0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Github Copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ddf5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "A popular AI-assisted programming tool is the Github Copilot for Visual Studio Code (VSCode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf162f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{seealso} LinkedIn Learning\n",
    "\n",
    "- [What is Github Copilot?][1]\n",
    "- [What is Github][2]\n",
    "::::\n",
    "\n",
    "[1]: https://www.linkedin.com/learning-login/share?account=76816450&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Fai-pair-programming-with-github-copilot%2Fwhat-is-copilot%3Ftrk%3Dshare_video_url%26shareId%3Dm1KXWA7HTqaZU1P8byqvgA%253D%253D\n",
    "[2]: https://www.linkedin.com/learning-login/share?account=76816450&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Flearning-github-18719601%2Fwhat-is-github%3Ftrk%3Dshare_video_url%26shareId%3Dl%252Bbl2Q4nSMuZ5%252BkgXmIcWg%253D%253D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8fbba3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "To get started, open the [setup guide](https://docs.github.com/en/copilot/setting-up-github-copilot/setting-up-github-copilot-for-yourself):\n",
    "\n",
    "1. Follow Step 1 last bullet point to [get free access as a verified student](https://docs.github.com/en/copilot/managing-copilot/managing-copilot-as-an-individual-subscriber/getting-free-access-to-copilot-as-a-student-teacher-or-maintainer).\n",
    "2. Instead of Step 2, you can access VSCode through the JupyterHub:\n",
    "   1. Click `File`->`New launcher`->`VS Code` to open VSCode.\n",
    "   2. Click the profile icon <i class=\"fas fa-user\"></i> on the left menu and select `Sign in with Github to use Github Copilot`.\n",
    "   3. After logging into Github, click the chat icon <i class=\"fas fa-comments\"></i> on the left menu of VSCode to start chatting.[^collapsed] \n",
    "3. Step 3 is for advanced user who would like to use Github Copilot in the command line interface (CLI), which is available in both the JupyterLab and VSCode interfaces. You may directly jump to the [step](https://docs.github.com/en/copilot/managing-copilot/configure-personal-settings/installing-github-copilot-in-the-cli#installing-copilot-in-the-cli) following the prerequisites.\n",
    "\n",
    "[^collapsed]: If your screen is not large enough, the icon may be collapsed into `...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd679e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{seealso} How to chat with Copilot effectively?\n",
    ":class: dropdown\n",
    "\n",
    "See the [prompt engineering guide](https://docs.github.com/en/copilot/using-github-copilot/prompt-engineering-for-github-copilot) for some suggestions.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabbf05e",
   "metadata": {},
   "source": [
    "You may also use a model served by Ollama as a substitute for Copilot. See the [`Continue` extension guide](https://docs.continue.dev/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef097ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Glossary\n",
    "\n",
    ":::::{admonition}\n",
    ":class: dropdown\n",
    "\n",
    "::::{glossary}\n",
    "\n",
    "line magic\n",
    ": A command in Jupyter notebook that starts with `%` such as `%load_ext` is called a [line magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-load_ext). It can be used to performs tasks to change the notebook behavior, such as loading the `%%ai` cell magic.\n",
    "\n",
    "cell magic\n",
    ": A command in Jupyter notebook that starts with `%%` such as `%%ai` is called a [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cell-magics). It can be used to change the beaviour of the cell, such as sending the text as a prompt to an LLM.\n",
    "\n",
    "shell capture\n",
    ": A command in Jupyter notebook that starts with `!` or `!!` is called a [shell capture](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-sc). It can be used to execute a shell/terminal command.\n",
    "\n",
    "REST API\n",
    ": REST API stands for Representational State Transfer Application Programming Interface. It is a set of rules for building and interacting with web services that use standard HTTP methods to enable communication between client and server.\n",
    "\n",
    "::::\n",
    ":::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
