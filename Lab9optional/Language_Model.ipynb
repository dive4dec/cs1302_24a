{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef337259",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "title: Language Model\n",
    "math:\n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert'\n",
    "    '\\orm': '\\left\\lvert #1 \\right\\rvert'\n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\set': '\\operatorname{set}'   \n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\hR': '\\R{\\hat{#1}}'\n",
    "    '\\RM': '\\mathbf{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\SFM': '\\operatorname{SFM}'\n",
    "    '\\utag': '\\stackrel{\\text{(#1)}}{#2}'\n",
    "    '\\uref': '\\text{(#1)}'\n",
    "    '\\minimal': '\\operatorname{minimal}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fe2e7",
   "metadata": {},
   "source": [
    "::::{attention}\n",
    "This notebook is optional and NOT required for any course assessment activities. Lab tutor may go through them if time is available.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import install_dependencies, show\n",
    "\n",
    "await install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON\n",
    "import transformers as tfm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a95771",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30128a77",
   "metadata": {},
   "source": [
    "What is a language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9f265",
   "metadata": {},
   "source": [
    "From [the wiki page](https://en.wikipedia.org/wiki/Language_model):\n",
    "\n",
    "> A language model is a probabilistic model of a natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799831b4",
   "metadata": {},
   "source": [
    "To put it simply, a causal language model completes an input prompt such as\n",
    "\n",
    "> A language model is ...\n",
    "\n",
    "into a realistic text like the one from the wiki page. More formally:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfee8fe",
   "metadata": {},
   "source": [
    "::::{prf:definition} language model\n",
    ":label: def:LM\n",
    "\n",
    "A language model is a generative (artificial neural) network trained on a *dataset* of *samples/examples* of *random text/source* $\\R{s}$ to generate realistic text $\\hR{s}$ when given a prompt $\\R{u}$ for $\\R{s}$. The goal is to make the conditional pmf $p_{\\hR{s}|\\R{u}}$ as close as possible to $p_{\\R{s}|\\R{u}}$ but without knowing knowing the joint distribution of $\\R{u}$ and $\\R{s}$. The statistical \"closeness\" can be measured by a [divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)) such as the Kullback-Leibler (information) [divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "$$\n",
    "D(\\R{s}\\|\\hR{s}|\\R{u}) = \\underbrace{\\E\\left[ \\log \\frac1{p_{\\hR{s}|\\R{u}}(\\R{s}|\\R{u})}\\right]}_{\\text{cross entropy $H(\\R{s}\\|\\hR{s}|\\R{u})$}} - \\underbrace{\\E\\left[\\log \\frac{1}{p_{\\R{s}|\\R{u}}(\\R{s}|\\R{u})} \\right]}_{\\text{entropy $H(\\R{s}|\\R{u})$}}.\n",
    "$$ (eq:D)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6976f89",
   "metadata": {},
   "source": [
    "::::{prf:remark} Training objective\n",
    "\n",
    "We denotes random variables in sanserif font. For instance, the entropy $H(\\R{s}|\\R{u})$ above is an expectation, denoted by $\\E[\\cdot]$, of the log reciprocal of the probability mass $p_{\\R{s}|\\R{u}}(\\R{s}|\\R{u})$, which is random because the arguments $\\R{s}$ and $\\R{u}$ are random.\n",
    "\n",
    "The divergence in [](#eq:D) is an important statistical distance in Information Theory and Machine Learning. Since the entropy $H(\\R{s}|\\R{u})$ does not depend on the generative network, i.e., $p_{\\hR{s}|\\R{u}}$, minimizing the divergence is equivalent to minimizing the cross entropy $H(\\R{s}\\|\\hR{s}|\\R{u})$, which is often used as an objective function in training a model.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f4685",
   "metadata": {},
   "source": [
    "For the divergence in [](#eq:D) to be called a divergence, it satisfies the following property:[^divergence]\n",
    "\n",
    "[^divergence]: The divergence is not a (pseudo)-metric as it is not symmetric, i.e., $D(\\R{s}\\|\\hR{s}|\\R{u})\\not\\equiv D(\\hR{s}\\|\\R{s}|\\R{u})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170f240",
   "metadata": {},
   "source": [
    "::::{prf:proposition} non-negativity of divergence\n",
    ":label: pro:D:non-negative\n",
    "\n",
    "The divergence in [](#eq:D) is non-negative, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(\\R{s}\\|\\hR{s}|\\R{u}) &\\geq 0, && \\text{or equivalently}\\\\\n",
    "H(\\R{s}\\|\\hR{s}|\\R{u}) &\\geq H(\\R{s}|\\R{u}).\n",
    "\\end{align}\n",
    "$$ (eq:D:non-negative)\n",
    "\n",
    "Equality holds if and only if the conditional distributions are identical almost surely, i.e., the random event[^random]\n",
    "\n",
    "$$\n",
    "p_{\\hR{s}|\\R{u}}(\\R{s}|\\R{u}) = p_{\\hR{s}|\\R{u}}(\\R{s}|\\R{u})\n",
    "$$ (eq:D:0)\n",
    "\n",
    "occurs with probability $1$.\n",
    "\n",
    "::::\n",
    "\n",
    "[^random]: [](#eq:D:0) is a random event because $\\R{u}$ and $\\R{s}$ are random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c218047",
   "metadata": {},
   "source": [
    "::::{prf:proof}\n",
    ":nonumber:\n",
    "\n",
    "To prove the non-negativity [](#eq:D:non-negative) of the divergence, we start from equation [](#eq:D):\n",
    "\n",
    "\\begin{align}\n",
    "\\E\\left[ \\log p_{\\R{s}|\\R{u}}(\\R{s}|\\R{u}) - \\log p_{\\hR{s}|\\R{u}}(\\R{s}|\\R{u}) \\right]\n",
    "&= \\E\\left[\\log \\frac{p_{\\R{s}|\\R{u}}(\\R{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\R{s}|\\R{u})} \\right]\\\\\n",
    "&\\utag{a}= \\E\\left[\\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})} \\log \\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})} \\right]\\\\\n",
    "&\\utag{b}\\geq \\E\\left[\\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})}\\right] \\log \\E\\left[\\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})} \\right]\\\\\n",
    "&\\utag{c}= \\E\\left[\\sum_{x} p_{\\R{s}|\\R{u}}(x|\\R{u}) \\right] \\log \\E\\left[\\sum_{x} p_{\\R{s}|\\R{u}}(x|\\R{u}) \\right]\\\\\n",
    "&\\utag{d}= 1\\cdot \\log 1 = 0,\n",
    "\\end{align}\n",
    "\n",
    "- $\\uref{a}$ and $\\uref{c}$ follow from the definition of expectation. To show $\\uref{a}$, note that the R.H.S. (with an appropriate choice of $f$) is\n",
    "  $$\n",
    "  \\E\\left[\\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})} f(\\R{u},\\R{s})\\right]\n",
    "  &= \\E\\left[\\sum_{x} \\sout{p_{\\hR{s}|\\R{u}}(x|\\R{u})} \\frac{p_{\\R{s}|\\R{u}}(x|\\R{u})}{\\sout{p_{\\hR{s}|\\R{u}}(x|\\R{u})}} f(\\R{u},x)\\right]\\\\\n",
    "  &= \\E\\left[\\sum_{x} p_{\\R{s}|\\R{u}}(x|\\R{u}) f(\\R{u},x)\\right]\\\\\n",
    "  &= \\E\\left[f(\\R{u},\\R{s})\\right],\n",
    "  $$\n",
    "  which gives the L.H.S. of $\\uref{a}$. $\\uref{c}$ can be shown similarly with $f(u,x)=1$.\n",
    "\n",
    "- $\\uref{b}$ results from applying [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) (see [](#lem:jensen) below) to the convex function $r \\mapsto r \\log r$.\n",
    "\n",
    "- Since $r \\mapsto r \\log r$ is strictly convex, the inequality holds with equality if and only if $\\frac{p_{\\R{s}|\\R{u}}(\\hR{s}|\\R{u})}{p_{\\hR{s}|\\R{u}}(\\hR{s}|\\R{u})}$ for some constant $C$ almost surely. However, $C$ must be 1, which implies [](#eq:D:0), as probability mass must sum to $1$ over all possible outcomes, i.e.,\n",
    "  $$\n",
    "  \\sum_x p_{\\R{s}|\\R{u}}(x|\\R{u}) = 1 = \\sum_x p_{\\hR{s}|\\R{u}}(x|\\R{u}),\n",
    "  $$ \n",
    "  which also justifies $\\uref{d}$. (Q.E.D.)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472569e9",
   "metadata": {},
   "source": [
    "The above proof relies on the strict convexity of $f(r):= r\\log(r)$, i.e.:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9006789",
   "metadata": {},
   "source": [
    "::::{prf:definition} convexity\n",
    "\n",
    "A function $f:\\mc{R} \\to \\mathbb{R}$ is convex if for all $r_1,r_2\\in \\mc{R}$,\n",
    "\n",
    "$$\n",
    "\\lambda f(r_0) + (1-\\lambda) f(r_1) \\geq f(\\lambda r_0 + (1-\\lambda) r_1) && \\forall \\lambda \\in [0,1].\n",
    "$$ (eq:convexity)\n",
    "\n",
    "$f$ is strictly convex when the above inequality is strict if and only if $r_1\\neq r_2$ and $\\lambda\\in (0,1)$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a0ea8",
   "metadata": {},
   "source": [
    "::::{prf:lemma} Jensen's inequality\n",
    ":label: lem:jensen\n",
    "\n",
    "For any random variable $\\R{r}$ and convex function $f$, i.e., [](#eq:convexity) holds, we have\n",
    "\n",
    "$$\n",
    "\\E[f(\\R{r})] \\geq f(\\E[\\R{r}]).\n",
    "$$ (eq:jensen)\n",
    "\n",
    "If $f$ is strictly convex, equality holds if and only if $f(\\R{r})$ is deterministic, i.e., equal to a constant almost surely.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c937d",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:jensen\n",
    "\n",
    "Prove the Jensen's inequality in [](#eq:jensen) for discrete random variable $\\R{r}$ taking values from a finte set $\\mc{R}$.[^jensen]\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "Consider a [proof by induction](https://en.wikipedia.org/wiki/Mathematical_induction) on the size $n$ of the support set \n",
    "\n",
    "$$\n",
    "\\operatorname{supp}(\\R{r}) := \\Set{r\\in \\mc{R} | p_{\\R{r}}(r)>0} = \\Set{r_i | i\\in [n]:=\\Set{0,\\dots,n-1}}.\n",
    "$$\n",
    "\n",
    "The base case with $n = 1$ is trivial. To show the inductive step, note that [](#eq:convexity) can be obtained from [](#eq:jensen) by setting \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{\\R{r}}(r_0) &= \\lambda\\\\\n",
    "p_{\\R{r}}(r_1) &= 1-\\lambda.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "[^jensen]: We have restricted Jensen's inequality to discrete random variables here for simplicity, even though the inequality hold more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159dde6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31d83a40085328d37de70081b4eaa81b",
     "grade": true,
     "grade_id": "jensen",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d996d8",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5b8d",
   "metadata": {},
   "source": [
    "Just like we compose a text using words from a vocabulary, a language model also generates a text from a vocabulary consisting of meaningful units called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a30502",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{prf:definition} tokenizer\n",
    "\n",
    "A tokenizer encodes an input text $x$ into a sequence\n",
    "\n",
    "$$\n",
    "y=(x_0, x_1, \\dots, x_{n-1}) := f(x) \\in \\mc{X}^n,\n",
    "$$ (eq:tokens)\n",
    "\n",
    "and decode the sequence back to $x = f^{-1}(y)$. Each $x_i$, called a token, takes value from the same set $\\mathcal{Y}$ of vocabulary.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6190c",
   "metadata": {},
   "source": [
    " The following code creates a tokenizer from the configuration files under `model_path` using [`AutoTokenizer.from_pretrained`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44899e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "model_path = \"/models/hf/Phi-3.5-mini-instruct/\"\n",
    "tokenizer = tfm.AutoTokenizer.from_pretrained(model_path)\n",
    "show(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5c19",
   "metadata": {},
   "source": [
    "The configurations of the tokenizer is specified in [JSON format](https://en.wikipedia.org/wiki/JSON), which is a collection of key/value pairs where the keys are names given as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abfbd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "JSON(filename=os.path.join(model_path, \"tokenizer_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e53af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "JSON(filename=os.path.join(model_path, \"special_tokens_map.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24071373",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "JSON(filename=os.path.join(model_path, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490af5f7",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "`AutoTokenizer` automatically selects a more specific tokenizer type such as `LlamaTokenizerFast` for the specified model.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a3792",
   "metadata": {},
   "source": [
    "To encode and decode a text using the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A language model is a probabilistic model of a natural language.\"\n",
    "ids = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "assert text == decoded_text\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(tokenizer.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(tokenizer.decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416743d",
   "metadata": {},
   "source": [
    "For efficient implementation of `encode` and `decode`, tokens are represented by integers known as the token ID's. The mapping from ID's to tokens is provided by the dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2143",
   "metadata": {},
   "source": [
    "To obtain the tokens from ID's, we can use the method `convert_ids_to_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a8aec",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: reverse_dict\n",
    "\n",
    "Write a function `reverse_dict(d)` to create a new dictionary where the keys are the values of the input dictionary `d`, and the values are the original keys. If multiple keys share the same value, only the last key should be kept. Assume the values of `d` are hashable.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00645bf9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f978f2026b62848e4bc18e1f727be5d",
     "grade": false,
     "grade_id": "reverse_dict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reverse_dict(d):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d16e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c142cec1e71f84c3dfc280e13c1da63a",
     "grade": true,
     "grade_id": "test-reverse_dict",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "reversed_vocab = reverse_dict(tokenizer.vocab)\n",
    "assert tokens == [reversed_vocab[i] for i in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91436478",
   "metadata": {},
   "source": [
    "Note that a token needs not be an English word. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[5], tokens[6], tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5f8db",
   "metadata": {},
   "source": [
    "The tokens can be punctuations such as `.` and even subwords that are meaningful by itself such as `▁probabil` and `isstic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5604b0b",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:meta\n",
    "\n",
    "Why some tokens such as `▁probabil` has a meta symbol `▁` (which is not the same an underscore `_`) but some does not such as `istic`?\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "See the [tokenization process](https://github.com/google/sentencepiece#whitespace-is-treated-as-a-basic-symbol).\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d73db7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f343ea3fc8c298005b7754081b423df",
     "grade": true,
     "grade_id": "meta",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78ee6f",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f593",
   "metadata": {},
   "source": [
    "A language model generates a text one token at a time just like we speak a text word-by-word. The model is probabilistic in the sense each token is generated randomly according some distribution. The sequence of randomly generated tokens is called a [*stochastic/random process*](https://en.wikipedia.org/wiki/Stochastic_process). If each token is generated independently based on some previously generated tokens, the process is said to be *auto-regressive*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204236d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{prf:definition} auto-regressive generation\n",
    "\n",
    "The generated text $\\hR{s}$ of a language model in [](#def:LM) is the decoding $\\hR{s} = f^{-1}(\\R{x})$ of the sequence of tokens $\\R{x}$ in [](#eq:tokens) where:\n",
    "\n",
    "- For some integer $n>0$ called the context length, the new tokens $\\R{x}_{n+t}$ for $t\\in \\mathbb{N}$ is sampled independently based on the realization of the last $n$ tokens $\\R{x}_{t:n+t}$ called the *context*, i.e.,\n",
    "\n",
    "  $$\n",
    "  p_{\\R{x}_{n+t}|\\R{x}_{:n+t}}(x_{n+t}|x_{:n+t}) = p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|x_{t:n+t})\n",
    "  $$ (eq:auto-regressive)\n",
    "  \n",
    "  for all $x_{:n+t+1}\\in \\mc{X}^{n+t+1}$.\n",
    "- the initial sequence of tokens is the sequence $\\R{x}_{:n} = f(\\R{s})$ of tokens for the input prompt $\\R{s}$.\n",
    "  \n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5100b",
   "metadata": {},
   "source": [
    "A potential confusion is to think that a token cannot depend on other tokens outside the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2172fb",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:auto-regressive\n",
    "\n",
    "Give a counter-example that a process satisfying [](#eq:auto-regressive) can have $\\R{x}_{n+t}$ depend on $\\R{x}_{:t}$.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "See the [data processing inequality](https://en.wikipedia.org/wiki/Data_processing_inequality) and [conditional independence](https://math.stackexchange.com/questions/22407/independence-and-conditional-independence-between-random-variables).\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2fbf3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69d7a1a33d0a4c174f793860cb490128",
     "grade": true,
     "grade_id": "auto-regressive",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be1c9b",
   "metadata": {},
   "source": [
    "If $\\R{s}$  a sequence of tokens shorter (longer) than the context length $n$, it can be left-padded (left-truncated) by the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9104638",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A language model is a probabilistic model of a natural language.\"\n",
    "encoding = tokenizer(text, padding='max_length', truncation=True)\n",
    "encoding.keys(), len(encoding.input_ids), len(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(tokenizer.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367fd58",
   "metadata": {},
   "source": [
    "The above call to `tokenizer` returns a dictionary consisting of two lists, both with the same length as the context length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df447ae6",
   "metadata": {},
   "source": [
    "`input_ids` points to the list of token ID's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(encoding.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28be79",
   "metadata": {},
   "source": [
    "Note the `input_ids` is left-padded by the padding token ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id, tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabab3d",
   "metadata": {},
   "source": [
    "Intuitively, the padding tokens should not be used to generate new tokens. To avoid unnecessary computations, the attention mask explicitly gives $0$ attention/importance/weight to those special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae47ae",
   "metadata": {},
   "source": [
    "There are also other special tokens that should normally be masked off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55129b7a",
   "metadata": {},
   "source": [
    "To load a language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = tfm.BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = tfm.AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available() and model.device.type != \"cuda\":\n",
    "    model = model.to(\"cuda\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83526965",
   "metadata": {},
   "source": [
    "A language model is a type of neural network consisting of layers of computational units called neurons. To generate text quickly, the above code attempts to utilize a Graphics Processing Unit (GPU) whenever available. It further quantizes the model to a lower precision, specifically 8-bit instead of the original 16-bit, to reduce the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f96b0",
   "metadata": {},
   "source": [
    "Finally, to generate the text, run the following cell:\n",
    "\n",
    "::::{tip}\n",
    "\n",
    "If it takes too long to generate, reduce `max_length` parameter to `50` or smaller.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59256b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text and generate output\n",
    "u = \"A language model is\"\n",
    "encoding = tokenizer(u, return_tensors=\"pt\")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available() and encoding.input_ids.device.type != 'cuda':\n",
    "    encoding = encoding.to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    shat_ids = model.generate(**encoding, max_length=100)\n",
    "\n",
    "# Decode the output\n",
    "shat = tokenizer.batch_decode(shat_ids)[0]\n",
    "print(shat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229406e",
   "metadata": {},
   "source": [
    "Note the repeatedly runing the above code will generate the same text. This is because, instead of sampling from the distribution in [](#eq:auto-regressive), it makes a hard decision:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cf313",
   "metadata": {},
   "source": [
    "::::{prf:definition} hardening\n",
    ":label: def:hard-decision\n",
    "\n",
    "A sequence $y^*$ is called the hard decision of $\\R{x}$  in [](#eq:auto-regressive) if it is one of the most probable sequence, i.e.,\n",
    "\n",
    "$$\n",
    "y^*_{n+t} \\in \\arg\\max_{x_{n+t}\\in \\mc{X}} p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|y^*_{t:n+1}),\n",
    "$$\n",
    "\n",
    "where [$\\arg\\max_{x_{n+t}}$](https://en.wikipedia.org/wiki/Arg_max) is the set of optimal solutions $x_{n+t}\\in \\mc{X}$ maximizing the conditional pmf.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744973",
   "metadata": {},
   "source": [
    "To perform the sampling, we can pass the keyword argument `do_sample=True` to `model.generate` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ae7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    shat_ids = model.generate(**encoding, max_length=100, do_sample=True)\n",
    "\n",
    "# Decode the output\n",
    "shat = tokenizer.batch_decode(shat_ids)[0]\n",
    "print(shat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c8d35",
   "metadata": {},
   "source": [
    "Verify that the code generates the tokens randomly by running it repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6e214",
   "metadata": {},
   "source": [
    "The generated text might have been cut off in the middle of a sentence. Although you can increase `max_length` to a sufficiently large value to ensure that generation terminates with an end-of-sequence token (`eos_token`), this can result in excessively long outputs. Fortunately, there are [other stopping criteria](https://huggingface.co/docs/transformers/v4.46.3/en/internal/generation_utils#transformers.StoppingCriteria) implemented that can help control the length and content of the generated text more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b5f04",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:stopping_criteria\n",
    "\n",
    "Modify the call to `model.generate` to stop at a line break `\"\\n\"`.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "Use `StoppingCriteriaList` and `StopStringCriteria` from `transformer`.\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d7ce7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a4d0bf0f959166d046ec63d6e858c18",
     "grade": false,
     "grade_id": "stopping_criteria",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign the desired stopping criteria to `stopping_criteria`.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "stopping_criteria\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    shat_ids = model.generate(**encoding, \n",
    "                              max_length=2000, # Make this big as the default is 20\n",
    "                              stopping_criteria=stopping_criteria\n",
    "                             )\n",
    "\n",
    "# Decode the output\n",
    "shat = tokenizer.batch_decode(shat_ids)[0]\n",
    "print(shat.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b2957",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b188c5a2f56925b09bd1d47d5eae8a5",
     "grade": true,
     "grade_id": "test-stopping_criteria",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "assert \"A language model is\" in shat\n",
    "assert \"\\n\" not in shat.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e420153",
   "metadata": {},
   "source": [
    "## Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74c08f",
   "metadata": {},
   "source": [
    "A language model can also be trained to complete a chat, following the [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT). The [Chat Completion API](https://huggingface.co/docs/api-inference/en/tasks/chat-completion#api-specification). A chat can be represented as a list of chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a25175",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI engineer who knows language models so well that you can explain the theory to a first-year undergraduate without any background.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a language model?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc457d",
   "metadata": {},
   "source": [
    "Each message is associated with a role:\n",
    "\n",
    "- The `system` message sets the behavior for the AI assistant.\n",
    "- The `user` message represents the user's query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79fd21",
   "metadata": {},
   "source": [
    "The tokenizer can be used to convert the list of messages into a single text for the language model to complete in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the chat template\n",
    "formatted_chat = tokenizer.applx_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Formatted chat:\\n\", formatted_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866134f4",
   "metadata": {},
   "source": [
    "Note that `<|system|>`, `<|user|>`, `<|assistant|>`, and `<|end|>` are special tokens used to mark the different chat messages. The chat template can be printed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = tokenizer.get_chat_template()\n",
    "print(\"Chat template:\\n\", chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0d5ed",
   "metadata": {},
   "source": [
    "This is a [Jinja](https://en.wikipedia.org/wiki/Jinja_(template_engine)) template, which uses python programming syntax such as iterations and conditionals to render the text from an input list `messages` of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b56a1",
   "metadata": {},
   "source": [
    "We can now call the language model to complete the text as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text and generate output\n",
    "u = formatted_chat\n",
    "encoding = tokenizer(u, return_tensors=\"pt\")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available() and encoding.input_ids.device.type != 'cuda':\n",
    "    encoding = encoding.to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    shat_ids = model.generate(**encoding, max_length=200)\n",
    "\n",
    "# Decode the output\n",
    "shat = tokenizer.batch_decode(shat_ids)[0]\n",
    "print(shat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52160e6c",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:decode_chat_messages\n",
    "\n",
    "Complete the following function that\n",
    "\n",
    "- takes an input list of tokens, obtained from a text in the chat template above, and\n",
    "- return a list of chat messages as dictionaries according to the Chat Completion API.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbf61a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8174ba100d6885d8dc8c44a7d1a3cff4",
     "grade": false,
     "grade_id": "decode_chat_messages",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def decode_chat_messages(ids):\n",
    "    roles = {32006: \"system\", 32010: \"user\", 32001: \"assistant\"}\n",
    "    output = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9009a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5ef7238c1344b474a86685c0d795c21",
     "grade": true,
     "grade_id": "test-decode_chat_messages",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "generated_text = \"\"\"\n",
    "<|system|> You are an AI engineer who knows language models so well that you can explain the theory to a first-year undergraduate without any background.<|end|>\n",
    "<|user|> What is a language model?<|end|>\n",
    "<|assistant|> A language model is a type of artificial intelligence (AI) system that is designed to understand, interpret, and generate human language. It is a mathematical representation of how words and phrases are likely to occur in a given language. Language models are used in various applications, such as speech recognition, machine translation, text generation, and natural language processing (NLP).\n",
    "\"\"\"\n",
    "\n",
    "assert decode_chat_messages(tokenizer.encode(generated_text)) == [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI engineer who knows language models so well that you can explain the theory to a first-year undergraduate without any background.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is a language model?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A language model is a type of artificial intelligence (AI) system that is designed to understand, interpret, and generate human language. It is a mathematical representation of how words and phrases are likely to occur in a given language. Language models are used in various applications, such as speech recognition, machine translation, text generation, and natural language processing (NLP).\\n\",\n",
    "    },\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
