{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f4e4d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "title: Transformer\n",
    "math:\n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert'\n",
    "    '\\norm': '\\left\\lVert #1 \\right\\rVert'\n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\set': '\\operatorname{set}'   \n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\hR': '\\R{\\hat{#1}}'\n",
    "    '\\RM': '\\mathbf{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\SFM': '\\operatorname{SFM}'\n",
    "    '\\utag': '\\stackrel{\\text{(#1)}}{#2}'\n",
    "    '\\uref': '\\text{(#1)}'\n",
    "    '\\minimal': '\\operatorname{minimal}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7eae9d",
   "metadata": {},
   "source": [
    "::::{attention}\n",
    "This notebook is optional and NOT required for any course assessment activities. Lab tutor may go through them if time is available.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7847f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import install_dependencies, show\n",
    "\n",
    "await install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON\n",
    "import ipywidgets as widgets\n",
    "from collections.abc import Iterable\n",
    "import transformers as tfm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732abfd",
   "metadata": {},
   "source": [
    "The success of large language models (LLM) can be attributed to \n",
    "\n",
    "1. the advancement of computing devices, such as Graphics Processing Units (GPUs),\n",
    "2. the availability of a large corpus of data, and\n",
    "3. the development of deep learning architectures and techniques\n",
    "\n",
    "that make it computationally feasible to train sophisticated language models on a large amount of data. In this notebook, we will introduce basic achitecture of LLM, which can be trained to capture important information for generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e331c2d",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e36cbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's visualize the training process! Click the play button (&#9654;) below to train a neural network that predicts the color of a point $(X_1,X_2)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ddd0a",
   "metadata": {},
   "source": [
    "::::{figure}\n",
    ":name: fig:neuron\n",
    "\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.82593&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
    ":width: 100%\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "A single neuron for a linearly separable dataset. [(open in new tab)](https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.82593&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb276e8",
   "metadata": {},
   "source": [
    "The output of the neural network is plotted as a heatmap:\n",
    "\n",
    "- The blue region is the *decision region* for classifying a point to be positive/blue.\n",
    "- The orange region is the decision region for classifying a point to be negative/orange.\n",
    "- The white line is a *decision boundary* separating the decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93bc6",
   "metadata": {},
   "source": [
    "The process of computing the output of a neural network is called the forward propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5976c92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{prf:example} forward propagation\n",
    "\n",
    "In [](#fig:neuron), the output of the neuron is computed as\n",
    "\n",
    "$$\n",
    "f_{\\theta}(X_1, X_2) := \\frac{1}{1+e^{-(w_0 X_1 + w_1 X_2 + b)}}\n",
    "$$\n",
    "\n",
    "from the input $(a_0, a_1)=(X_1, X_2)$ by\n",
    "\n",
    "1. taking their affine combination\n",
    "   \n",
    "   $$ (a_0, a_1) \\mapsto w_0 a_0 + w_1 a_1 + b $$ (eq:neuron:affine)\n",
    "   using the parameters $\\theta:=(w_0, w_1, b)$ consisting of\n",
    "   - the weight $w:=(w_0, w_1)\\in \\mathbb{R}^2$, and\n",
    "   - the bias $b\\in\\mathbb{R}$; and\n",
    "2. passing affine combination as input to a non-linear activation function such as the sigmoid function\n",
    "   \n",
    "   $$\n",
    "   z\\mapsto \\frac{1}{1+e^{-z}}.\n",
    "   $$ (eq:neuron:sigmoid)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a6bbe",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "The non-linear activation function is not necessary for linearly separable data, i.e., when the decision boundary can be linear such as the one in [](#fig:neuron). Non-linearity is needed to model more complex patterns, and multiple neurons can be interconnected to produce a better fit to the data.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7ceeb",
   "metadata": {},
   "source": [
    "For accurate prediction, the neural network is trained by examples called the *training set* to minimize an objective function called the *loss function*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756742",
   "metadata": {},
   "source": [
    "::::{prf:example} loss function\n",
    "\n",
    "The parameters are chosen/trained to minimize a loss function such as the mean squared error\n",
    "\n",
    "$$\n",
    "\\R{L}(\\theta) := \\frac1n \\sum_{i\\in [n]} \\left(f_{w_0, w_1, b}(\\R{X}_{i,1}, \\R{X}_{i,2}) - \\R{Y}_i\\right)^2\n",
    "$$ (eq:neuron:loss)\n",
    "\n",
    "Where $X_i :=(\\R{X}_{i,1}, \\R{X}_{i,2})$ is a training example with a known class value $\\R{Y}_i$ equal to \n",
    "\n",
    "- $1$ for positive examples (blue points); and\n",
    "- $0$ for negative examples (orange points).\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909aee81",
   "metadata": {},
   "source": [
    "The optimization is done step-by-step using a numerical method called the gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b755d",
   "metadata": {},
   "source": [
    "::::{prf:example} gradient descent\n",
    "\n",
    "The computed sequence of parameters after the $i$ iterations of the gradient descent is\n",
    "\n",
    "$$\n",
    "\\theta_{i+1} := \\theta_{i} - \\delta \\nabla \\R{L}(\\theta_i)\n",
    "$$\n",
    "\n",
    "where $\\theta_0$ is often initialized randomly using a normal distribution, $\\delta\\in \\mathbb{R}$ is called the step size, and \n",
    "\n",
    "$$\n",
    "\\nabla \\R{L}(\\theta) := \\left(\\frac{\\partial \\R{L}(\\theta)}{\\partial w_0}, \\frac{\\partial \\R{L}(\\theta)}{\\partial w_1}, \\frac{\\partial \\R{L}(\\theta)}{\\partial b}\\right),\n",
    "$$\n",
    "\n",
    "called the gradient, is the direction of the largest slope of $\\R{L}$ with respect to $\\theta$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286d49e",
   "metadata": {},
   "source": [
    "Traing speed and stability depends on the choice of the activation functi9n. Another common non-linear activation function is ReLU (Rectified Linear Unit) defined as\n",
    "\n",
    "$$\n",
    "z\\mapsto \\max(0, z).\n",
    "$$\n",
    "\n",
    "ReLU makes training faster as the slope does not diminish at the tails of the function. Sigmoid makes training more stable as it is smooth. SiReLU (Sigmoid-Rectified Linear Unit) is another activation function that combine the benefits of both. It can be defined as\n",
    "\n",
    "$$\n",
    "z \\mapsto \\frac{z}{1+e^{-z}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90be31",
   "metadata": {},
   "source": [
    "To classify more complicated data, a neural network can have more neurons arranged in modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d44ed",
   "metadata": {},
   "source": [
    "::::{prf:definition} Multi-layer perceptron\n",
    "\n",
    "An $k$-layer perceptron is a neural network where the output of layer $\\ell\\in [1:k+1]$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{\\ell}&:= \\sigma_{\\ell}(a_{\\ell-1}W_{\\ell}+b_{\\ell}) \\in \\mathbb{R}^{n_{\\ell}}\n",
    "\\end{align}\n",
    "$$ (eq:MLP)\n",
    "\n",
    "where\n",
    "\n",
    "- $a_0$ is the input layer containing the input values;\n",
    "- $W_{\\ell}$ is a matrix of weights;\n",
    "- $b_{\\ell}$ is a vector of biases; and\n",
    "- $\\sigma_{\\ell}$ is a vectorized *activation function*.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517432eb",
   "metadata": {},
   "source": [
    "::::{figure}\n",
    ":name: fig:spiral\n",
    "\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,2&seed=0.83213&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
    ":width: 100%\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "A more complex neural network and dataset. [(open in new tab)](https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,2&seed=0.83213&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f9edb",
   "metadata": {},
   "source": [
    "::::{tip} Tensorflow playground\n",
    "\n",
    "The above app is a slight modification of the open source project [Tensorflow Playground](https://playground.tensorflow.org) with the additional features that:\n",
    "- You can save your configuration to the browser session by clicking the button `Save to browser session`. If you reopen the browser, it will load your previously saved configuration automatically.\n",
    "- You can reset the configuration by clicking the `Reset` button.\n",
    "- The last button `Copy permalink to clipboard` copies the permalink to your configuration to the clipboard. You can save/share multiple configurations permanently using their the permalinks.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423dc69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:1\n",
    "\n",
    "In the tensorflow playground, choose the spiral data set from the data column and try to classify it well by adding more features, neurons, and hidden modules. Explain how you design your neural network and include a screen capture in the following cell such as:\n",
    "\n",
    "![spiral](images/spiral.png)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d865f90",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf354c31102717ea7a9b6bd78ec524f3",
     "grade": true,
     "grade_id": "spiral",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9ef66",
   "metadata": {},
   "source": [
    ":::::{seealso} What is a neural network?\n",
    ":class: dropdown\n",
    "\n",
    "\n",
    "\n",
    "::::{figure}\n",
    ":name: fig:manim_DL1\n",
    "\n",
    ":::{iframe} https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\n",
    ":width: 100%\n",
    ":::\n",
    "\n",
    "\n",
    "What is a neural network? [(open in new tab)](https://www.youtube.com/embed/aircAruvnKk?start=649&end=695)\n",
    "\n",
    "::::\n",
    "\n",
    ":::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046d118",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75b694",
   "metadata": {},
   "source": [
    "Recall the following code which \n",
    "\n",
    "1. creates a tokenizer from the configuration files under `model_path` using [`AutoTokenizer.from_pretrained`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained), and\n",
    "2. load the language model, using GPU whenever possible, and quantizes it to 8-bit for each parameter to reduce the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "model_path = \"/models/hf/Phi-3.5-mini-instruct/\"\n",
    "tokenizer = tfm.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model\n",
    "bnb_config = tfm.BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = tfm.AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available() and model.device.type != \"cuda\":\n",
    "    model = model.to(\"cuda\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb80bed",
   "metadata": {},
   "source": [
    "The model is composed on modules of interconnected modules.\n",
    "\n",
    "```\n",
    "Model loaded on device: cuda:0\n",
    "Phi3ForCausalLM(\n",
    "  (model): Phi3Model(\n",
    "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
    "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x Phi3DecoderLayer(\n",
    "        (self_attn): Phi3SdpaAttention(\n",
    "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
    "          (qkv_proj): Linear8bitLt(in_features=3072, out_features=9216, bias=False)\n",
    "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Phi3MLP(\n",
    "          (gate_up_proj): Linear8bitLt(in_features=3072, out_features=16384, bias=False)\n",
    "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
    "          (activation_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
    "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
    "      )\n",
    "    )\n",
    "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe26a88",
   "metadata": {},
   "source": [
    "The source code can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py). To inspect the input and output of each module, we can register a function, called a *hook*, to be executed whenever a module computes an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ab7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of handles of registered hook\n",
    "try:\n",
    "    handles  # avoid overwriting if defined\n",
    "except NameError:\n",
    "    handles = []  # list of handles to the hook\n",
    "\n",
    "# A hook to run every time after the forward method has computed an output\n",
    "def hook(module, args, output):\n",
    "    modules.append({\"module\": module, \"args\": args, \"output\": output})\n",
    "\n",
    "# Register the forward hook to each module\n",
    "modules = [*model.modules()]\n",
    "for module in modules:\n",
    "    handles.append(module.register_forward_hook(hook))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700b3c9",
   "metadata": {},
   "source": [
    "To generate the a new token and record the outputs of the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2013a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list that stores the modules and their inputs and outputs\n",
    "modules = []\n",
    "\n",
    "# Tokenize input text and generate output\n",
    "u = \"A language model is\"\n",
    "encoding = tokenizer(u, return_tensors=\"pt\")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available() and encoding.input_ids.device.type != 'cuda':\n",
    "    encoding = encoding.to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    shat_ids = model.generate(**encoding, max_new_tokens=1)\n",
    "\n",
    "# Decode the output\n",
    "shat = tokenizer.batch_decode(shat_ids)[0]\n",
    "print(shat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84c2d6",
   "metadata": {},
   "source": [
    "To display the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa88751",
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(i=widgets.Dropdown(\n",
    "    options=[(str(i)+': '+repr(modules[i]['module']),i) for i in range(len(modules))],\n",
    "    value=2,\n",
    "    description='Module:',\n",
    "    layout=widgets.Layout(width=\"90%\")    \n",
    "))\n",
    "def show_module(i):\n",
    "    def show_values(seq):\n",
    "        for v in seq:\n",
    "            print(' '*2 + str(type(v)) + (isinstance(v, torch.Tensor) and f' with shape: {v.shape}' or ''))\n",
    "            show(v)\n",
    "    print(\"Module:\")\n",
    "    show(modules[i]['module'])\n",
    "    print(\"Input(s):\")\n",
    "    show_values(modules[i]['args'])\n",
    "    print(\"Output(s):\")\n",
    "    outputs = modules[i]['output']\n",
    "    if not isinstance(outputs, Iterable):\n",
    "        outputs = [outputs]\n",
    "    show_values(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03746e4c",
   "metadata": {},
   "source": [
    "To clean up the hooks so they do not get executed again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c69256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: Remove the added hook\n",
    "for handle in handles:\n",
    "    handle.remove()\n",
    "handles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca4094",
   "metadata": {},
   "source": [
    "### Logits in Final Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80681f2b",
   "metadata": {},
   "source": [
    "Recall that an auto-regressive language model uses the conditional distribution $p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|x_{t:n+t})$ to generate the new token $\\R{x}_{n+t}$ from an existing sequence $x_{t:n+t}$ of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080471d5",
   "metadata": {},
   "source": [
    "The last registered output of the model contains the *logits*\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l := \\left[\\log p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|x_{t:n+t}) + c\\right]_{x_{n+t}\\in \\mc{X}},\n",
    "\\end{align}\n",
    "$$ (eq:logits)\n",
    "\n",
    "which are the log likelihood probabilities $\\log p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(\\cdot|x_{t:n+t})$ shifted by some constant $c\\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = modules[-1][\"output\"].logits\n",
    "show(logits.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf2981",
   "metadata": {},
   "source": [
    "The logits is the output of the last layer of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb23dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_module(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea12a7",
   "metadata": {},
   "source": [
    "By default, the new token to generate is obtained by hardening the logits directly as follows. There\n",
    "is no need to compute the likelihood probabilities first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf912db",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id = torch.argmax(logits, dim=-1)\n",
    "print(tokenizer.batch_decode(next_token_id)[0])\n",
    "next_token_id, tokenizer.convert_ids_to_tokens(next_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c2d68",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:logits\n",
    "\n",
    "Explain why\n",
    "\n",
    "$$\n",
    "\\arg\\max_{x_{x_n+}\\in \\mc{X}} \\log p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|x_{t:n+t}) + c\n",
    "= \\arg\\max_{x_{x_n+}\\in \\mc{X}} p_{\\R{x}_{n+t}|\\R{x}_{t:n+t}}(x_{n+t}|x_{t:n+t})\n",
    "$$\n",
    "\n",
    "for all $x_{t:n+t}\\in \\mathbb{R}^n$?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7357b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2ef580029d295cf470ffdc11b8dbe9c",
     "grade": false,
     "grade_id": "logits",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "::::{solution} ex:logits\n",
    ":class: dropdown\n",
    "\n",
    "This is because $\\log(\\cdot) + c$ is strictly increasing.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee71d7f",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:softmax\n",
    "\n",
    "The probabilities can be computed using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) defined as\n",
    "\n",
    "$$\n",
    "\\sigma(l) := \\frac{1}{\\sum_{i=1}^n e^{l_i}}\\begin{bmatrix} e^{l_1} & \\cdots & e^{l_n} \\end{bmatrix}.\n",
    "$$ (eq:softmax)\n",
    "\n",
    "Assign to `p` a tensor with the same dimension as `logits` but containing the conditional probability masses instead of logits.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "Use `torch.nn.functional.softmax` with the keyword argument `dim=-1`.\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f7426",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a0bf330aceb3cad61ddb7e4a362a03",
     "grade": false,
     "grade_id": "softmax",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "p\n",
    "show(p.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3393d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6a5208ec54514af462064117cab28a3",
     "grade": true,
     "grade_id": "test-softmax",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "# check whether p is stochastic.\n",
    "assert ((0 <= p) & (p <= 1)).all() and (p.sum(dim=-1) == 1).all()\n",
    "assert (torch.argmax(p, dim=-1) == torch.argmax(logits, dim=-1)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9ab35",
   "metadata": {},
   "source": [
    "### Embedding in First Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeff64",
   "metadata": {},
   "source": [
    "The first layer of the model is called the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_module(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856316e",
   "metadata": {},
   "source": [
    "The layer uses a pretrained embedding function to embed each token into a high-dimensional vector space:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c5233",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{prf:definition} Embedding\n",
    "\n",
    "An input sequence $x_{t:n+t}\\in \\mathbb{R}^n$ of token is represented by a matrix\n",
    "\n",
    "$$\n",
    "X :=\n",
    "\\begin{bmatrix}\n",
    "X_{0,0} & X_{0,1} & \\cdots & X_{0,d-1} \\\\\n",
    "\\vdots \\\\\n",
    "\\color{blue}X_{i, 0} & \\color{blue}X_{i, 1} & \\color{blue}\\cdots & \\color{blue}X_{i, d-1} \\\\\n",
    "\\vdots \\\\\n",
    "X_{n-1, 0} & X_{n-1, 1} & \\cdots & X_{n-1, d-1}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n\\times d},\n",
    "$$ (eq:embedding)\n",
    "\n",
    "where each token $x_{t+i} \\in \\mathcal{X}$ for $i\\in [n]$ is represented by the real vector, called the embedding of $x_{t+i}$, in the $i$-th row\n",
    "\n",
    "$$\n",
    "X_{i,:} = \n",
    "\\begin{bmatrix}\n",
    "\\color{blue}X_{i,0} & \\color{blue}X_{i,1} & \\color{blue}\\cdots & \\color{blue}X_{i,d-1}\n",
    "\\end{bmatrix}  := g(x_{t+i}) \\in \\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1f803",
   "metadata": {},
   "source": [
    "$g$ is the embedding function that embeds a token into the $d$-dimensional vector space. It is pretrained such that the distances between the embeddings of different tokens capture the differences in the meanings of the tokens. For instance, consider the embeddings of 'dog', 'cat', and 'car':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(tokenizer.encode('dog cat car'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = modules[0]['module'].forward(token_ids)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213f5a5",
   "metadata": {},
   "source": [
    "Dog is more similar to cat than to car using the [cosine similarlity](https://en.wikipedia.org/wiki/Cosine_similarity):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(a,b)\\in \\mathbb{R}^{d}\\times \\mathbb{R}^{d} \\mapsto \\frac{ab^{\\intercal}}{\\norm{a}\\norm{b}} &:= \\frac{\\sum_{i\\in [d]} a_i b_i}{\\sqrt{\\sum_{i\\in [d]} a_i^2} \\sqrt{\\sum_{i\\in [d]} b_i^2}}\\\\\n",
    "= \\cos \\theta_{ab}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\theta_{ab}$ is the angle between the vectors $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog vs cat\n",
    "torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63413b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog vs car\n",
    "torch.nn.functional.cosine_similarity(embeddings[0], embeddings[2], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd0c32",
   "metadata": {},
   "source": [
    "Note that `cosine_similarity` is a universal function, so we can compute the similarities in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(embeddings[0], embeddings, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8439765",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: similarity_matrix\n",
    "\n",
    "Assign to `similarity_matrix` a 3-by-3 tensor where the entry at `[i, j]` is the similarity between row `embeddings[i]` and `embeddings[j]`.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "You can call cosine_similarity just once by reshape `embeddings` before passing as arguments.\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f290e7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a8c42fc777ad0ccd0f7742afa701675",
     "grade": false,
     "grade_id": "similarity_matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc7392",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38424b1d404590f58db7fc660b5f3596",
     "grade": true,
     "grade_id": "test-similarity_matrix",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert (\n",
    "    similarity_matrix.cpu() ** 2\n",
    "    - torch.tensor(\n",
    "        [[1.0000, 0.0533, 0.0157], [0.0533, 1.0000, 0.0177], [0.0157, 0.0177, 1.0000]]\n",
    "    ).abs()\n",
    "    < 1e-4\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a54d0",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783cd0c",
   "metadata": {},
   "source": [
    "An important component of the transformer architecture is the attention mechanism proposed by [Vaswani et al. 2017](https://doi.org/10.48550/arXiv.1706.03762), which can be efficiently trained and computed to focus on the most relevant parts of the context, which can be longer than those of the traditional sequential architectures such as [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) and [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a100846",
   "metadata": {},
   "source": [
    "For the current mode, there is a stack 32 decoder modules, each of which implements the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_module(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df61c0",
   "metadata": {},
   "source": [
    "The attention function is defined as follows and the source code of the attention module can be found [here](https://github.com/huggingface/transformers/blob/6c3f168b36882f0beebaa9121eafa1928ba29633/src/transformers/models/phi3/modeling_phi3.py#L555)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea74a4",
   "metadata": {},
   "source": [
    "::::{prf:definition} Attention Layer\n",
    "\n",
    "Define the attention function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(Q,K,V) &:= \\sigma\\left(\\frac1{\\sqrt{d}}QK^{\\intercal}\\right)V\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the softmax function in [](#eq:softmax), and\n",
    "\n",
    "-  $Q\\in \\mathbb{R}^{n \\times d}$ is called the query matrix;\n",
    "-  $K\\in \\mathbb{R}^{n \\times d}$ is called the key matrix; and\n",
    "-  $V\\in \\mathbb{R}^{d \\times m}$ is called the value matrix.\n",
    "\n",
    "A (multihead) attention layer is the attention function parameterized by the weight $W$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_{W}(Q, K, V) &:= \\sum_{i\\in [h]} \\alpha(QW_{0,i}, K W_{1,i},VW_{2,i}) W_{3,i},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "- $d=l=m \\times h$ for some $m$ and $h$; and\n",
    "- for $i\\in [h]:=\\Set{0,\\dots,h-1}$,\n",
    "    - $W_{0,i}, W_{1,i}, W_{2,i} \\in \\mathbb{R}^{d\\times m}$ and\n",
    "    - $W_{3,i} \\in \\mathbb{R}^{m \\times l}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a65c1",
   "metadata": {},
   "source": [
    "The query $Q$, key $K$, and value $V$ are derived from the input embeddings $X$, as in $\\mu_{W}(X, X, X)$. Specifically, $Q$ and $K$ are used to calculate attention scores, while $V$ holds the actual values to be attended to based on these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a8780",
   "metadata": {},
   "source": [
    "Note that the different rows of $Q$ go through the same linear transformations, i.e., the position information is immaterial in the calculation of the attention score. In order to weight the importance of a token in the context based on its position relative the new token to be generated, an additional positional encoding is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_module(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f221b5",
   "metadata": {},
   "source": [
    "An example is the rotary positional encoding (not the one used above) proposed by [Su et al. 2021](https://doi.org/10.48550/arXiv.2104.09864) is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4a977",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{prf:definition} rotary positional encoding\n",
    "\n",
    "Define the rotation matrix \n",
    "\n",
    "$$\n",
    "R(\\theta)\n",
    "&:= \\begin{bmatrix}\n",
    "\\cos(\\theta) & \\sin(\\theta) \\\\\n",
    "-\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}.\n",
    "$$ (eq:rotation)\n",
    "\n",
    "The rotary positional encoding of an embedding $X\\in \\mathbb{R}^{n\\times d}$ is a matrix $Z \\in \\mathbb{R}^{n\\times d}$ with the same dimension as $X$ such that row $i\\in [n]$ is encoded as\n",
    "\n",
    "$$\n",
    "Z_{i,:} = \n",
    "\\begin{bmatrix}\n",
    "Z_{i,0} & Z_{i,1} & \\cdots & \\color{blue}Z_{i,2j} & \\color{blue}Z_{i,2j+1} & \\cdots & Z_{i,d-2} & Z_{i,d-1}\n",
    "\\end{bmatrix}  := g_i(X_{i,:}) \\in \\mathbb{R}^d,\n",
    "$$\n",
    "\n",
    "where, for $j\\in [d/2]$ (with $d$ chosen to be even),\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\color{blue}Z_{i,2j} & \\color{blue}Z_{i,2j+1} \n",
    "\\end{bmatrix}\n",
    "&:=\n",
    "\\begin{bmatrix}\n",
    "X_{i,2j} & X_{i,2j+1} \n",
    "\\end{bmatrix} R(i \\theta_j) && \\text{and}\\\\\n",
    "\\theta_j &:= b^{-2j/d}\n",
    "\\end{align}\n",
    "$$ (eq:rotary)\n",
    "\n",
    "for some positive integer base $b$, and $R$ is the rotation matrix in [](#eq:rotation).\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c8d57",
   "metadata": {},
   "source": [
    "$Z$ or its linear transformations is passed to the attention function as in $\\mu_{W}(Z, Z, Z)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
